{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6dd074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend module://matplotlib_inline.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from vits.utils.utils import load_wav_to_torch\n",
    "from vits.utils.mel_processing import spectrogram_torch\n",
    "\n",
    "from vits.model import commons\n",
    "from vits.utils import utils\n",
    "from vits.model.models import SynthesizerTrn\n",
    "from vits.text.symbols import symbols\n",
    "from vits.text import cleaned_text_to_sequence, text_to_sequence, batch_text_to_sequence\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "def get_text(text, hps, language_code):\n",
    "    text_norm = text_to_sequence(text, str(language_code))\n",
    "    if hps.data.add_blank:\n",
    "        text_norm = commons.intersperse(text_norm, 0) \n",
    "    text_norm = torch.LongTensor(text_norm)\n",
    "    return text_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e6a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068b70fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/envs/vits_pl/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "hps = utils.get_hparams_from_file(\"vits/configs/vits_base.json\")\n",
    "\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "net_g = SynthesizerTrn(\n",
    "    len(symbols),\n",
    "    hps.data.filter_length // 2 + 1,\n",
    "    hps.train.segment_size // hps.data.hop_length,\n",
    "    n_speakers=61,\n",
    "    **hps.model).to(device)\n",
    "_ = net_g.eval()\n",
    "\n",
    "# _ = utils.load_checkpoint(\"vits/checkpoints/vits_pl_test/G_?.pth\", net_g, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e5a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
     ]
    }
   ],
   "source": [
    "def dataCollate(txt_lists):\n",
    "    max_text_len = max([len(x) for x in txt_lists])\n",
    "    text_lengths = torch.LongTensor(len(txt_lists))\n",
    "    text_padded = torch.LongTensor(len(txt_lists), max_text_len)\n",
    "    text_padded.zero_()\n",
    "\n",
    "    for i in range(len(txt_lists)):\n",
    "        text = torch.LongTensor(txt_lists[i])\n",
    "        text_padded[i, :len(text)] = text\n",
    "        text_lengths[i] = len(text)\n",
    "\n",
    "    return text_padded, text_lengths\n",
    "\n",
    "def find_mask(audio):\n",
    "    mask = 0\n",
    "    for i in range(len(audio)-1, 0, -1):\n",
    "        if torch.abs(audio[i]) < 0.01 and torch.mean(torch.abs(audio[i-100:i])) < 0.001:\n",
    "            mask = i\n",
    "            break\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def concat_audio(audio_list):\n",
    "    final_audio = []\n",
    "\n",
    "    for audio in audio_list:\n",
    "        mask = find_mask(audio[0].data)\n",
    "        final_audio.append(audio[0].data[:mask])\n",
    "\n",
    "    final_audio = torch.cat(final_audio, dim=0)\n",
    "    return final_audio\n",
    "\n",
    "language_code = 0\n",
    "group_size = 5\n",
    "input_text = \"반갑습니다. 당신의 상담사로 이곳에 오게 되었습니다. 당신의 불편한 점을 말씀해주시겠어요?\"\n",
    "txt_lists = batch_text_to_sequence(input_text, str(language_code), group_size)\n",
    "text_padded, text_lengths = dataCollate(txt_lists)\n",
    "\n",
    "sid=None\n",
    "lid=None\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    x_tst = text_padded.to(device)\n",
    "    x_tst_lengths = text_lengths.to(device)\n",
    "\n",
    "    sid = torch.LongTensor([0]).to(device)\n",
    "#     audio = net_g.infer(x_tst, x_tst_lengths, sid=sid, lid=lid, noise_scale=0.333, noise_scale_w=0.1, length_scale=1.2)\n",
    "\n",
    "# final_audio = concat_audio(audio[0]).cpu().float().numpy()\n",
    "# ipd.display(ipd.Audio(final_audio, rate=hps.data.sampling_rate, normalize=False))\n",
    "\n",
    "# from scipy.io.wavfile import write\n",
    "# write(\"/workspace/test.wav\", 22050, final_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6115bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vits_pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
